---
title: "AoA prediction template"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

# load libraries
library(tidyverse)
library(modelr)
library(glue)
library(wordbankr)
library(childesr)

# load functions
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Load Wordbank data

Loading cached Wordbank data for English:
```{r load_wordbank_eng}
eng_wb_data <- load_wb_data("English (American)")
eng_wb_data
```

Loading Wordbank data for multiple languages (cached or not):
```{r load_wordbank_xling}
target_langs <- c("English (American)", "Norwegian","Russian", 
                  "Turkish", "Italian", "Swedish",  
                  "French (Quebecois)", "Danish", "Croatian", 
                  "French (French)", "English (Australian)","German", 
                  "English (British)", "Spanish (European)", "Spanish (Mexican)",
                  "Portuguese (European)", "Mandarin (Beijing)","Mandarin (Taiwanese)")

sinotibetan_languages <- c("Mandarin (Taiwanese)", "Mandarin (Beijing)")


wb_data <- load_wb_data(target_langs)
```

Creating saved Wordbank data for a language, for example:
```{r create_wordbank_eng, eval=FALSE}
#eng_wb_data <- create_wb_data("English (American)")
```

Creating saved Wordbank data one step at a time (potentially making changes between steps), for example:
```{r create_wordbank_eng_steps, eval=FALSE}
#eng_wg <- create_inst_data("English (American)", "WG")
#eng_ws <- create_inst_data("English (American)", "WS")
#eng_wg_summary <- collapse_inst_data(eng_wg)
#eng_ws_summary <- collapse_inst_data(eng_ws)
#eng_comb_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
```


# Load predictors

## Ratings and phonemes

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r merge_unilemmas}
uni_lemmas <- extract_uni_lemmas(wb_data)
```

```{r load_predictors}
babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas |> map_predictor("babiness", babiness_map)

valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas |> map_predictor("valence", valence_map)

concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```

## CHILDES

Loading cached CHILDES metrics for English:
```{r load_childes_eng}
#eng_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Loading cached CHILDES data for multiple languages:
```{r load_childes_xling}
childes_metrics <- load_childes_metrics(target_langs, uni_lemmas) |>
  filter(!is.na(uni_lemma))
  
```

Creating saved CHILDES metrics for English, potentially changing which metrics are computed and/or arguments that are passed to `childesr` functions:
```{r specify_childes, eval=FALSE}
metric_funs <- list(compute_count, compute_mlu, compute_positions,
                    compute_length_char, compute_length_phon)
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")
#eng_childes <- get_token_metrics("English (American)", metric_funs, corpus_args)
#eng_unilemmas <- get_uni_lemma_metrics("English (American)", build_uni_lemma_map(uni_lemmas))

#childes_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Creating saved CHILDES data for many languages:
```{r create_childes_xling, eval=FALSE}
#walk(target_langs, get_token_metrics, metric_funs, corpus_args)
#walk(target_langs, load_childes_metrics, uni_lemmas)
```

```{r}
# TODO: get phonology for tokens that didn't get it from CHILDES
# http://espeak.sourceforge.net
# phonemes <- uni_lemmas |> map_phonemes()
# left_join(phonemes, by = c("language", "uni_lemma"))
```


# Prepare data for modeling

## Frequency transformations

By default, `transform_counts()` transforms any column that starts with "count" by smoothing (add 1), normalizing, and log transforming, then renaming every column "count_x" to "freq_x". `residualize_freqs()` residualizes all columns that starts with "freq_" from the column "freq".

```{r prepare_frequency}
childes_metrics <- childes_metrics |> transform_counts() |> residualize_freqs() 
```

## Combining sources

Combine mapped predictors and CHILDES predictors:

```{r merge_all}
predictor_data_list <- list(babiness, valence, concreteness, childes_metrics)
predictor_data <- predictor_data_list |>
  reduce(partial(full_join, by = c("language", "uni_lemma"))) 
```

## Set lexical category contrasts

```{r}
predictor_data_lexcat <- prep_lexcat(predictor_data, uni_lemmas)
```

## Imputation

```{r impute_data}
predictor_sources <- list(
  c("freq", "freq_last", "freq_first", "freq_solo", "mlu"),
  c("arousal", "valence"),
  "concreteness",
  "babiness",
  "length_char",
  # "length_phon",
  "n_types"
)
predictors <- unlist(predictor_sources)

predictors_short<- c("freq", "freq_first", "mlu", "valence", "babiness", "babiness", "freq_last", "freq_solo", "arousal", "concreteness")


predictor_data_imputed <- predictor_data_lexcat |>
  do_full_imputation(predictor_sources, max_steps = 20)
```

## Scaling

```{r}
predictor_data_scaled <- do_scaling(predictor_data_imputed, predictors)
```

# Fit models

```{r aoa-lm}
aoas <- fit_aoas(wb_data)
aoa_predictor_data <- aoas |> left_join(predictor_data_scaled)

aoa_predictor_data <- aoa_predictor_data |>
  filter(!(language == "Russian" & category == "sounds")) |>
  filter(!(aoa>48)) |>
  filter(!is.na(uni_lemma)) |>
  mutate(length_char = ifelse (language %in% sinotibetan_languages, NA, length_char)) |>
  mutate(n_types = ifelse (language %in% sinotibetan_languages, NA, n_types))

aoa_models <- fit_models(predictors, aoa_predictor_data)
```

## Model outputs

Coefficients:
```{r}
aoa_models |> select(language, measure, coefs) |> unnest(coefs)
```

Summary stats:
```{r}
aoa_models |> select(language, measure, stats) |> unnest(stats)
```

Variance inflation factors:
```{r}
aoa_models |> select(language, measure, vifs) |> unnest(vifs)
```


## Cross-validation

```{r cross_validate}
# for simple cv result extraction
loo_df <- aoa_predictor_data |>
  group_nest(language, measure) |>
  mutate(loo_models = map(data, fit_cv_models,
                          list(make_predictor_formula(predictors))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))

cv_results <- loo_df |>
  select(language, measure, cv_results) |>
  unnest(cv_results)

loo_preds <- loo_df |>
  select(language, measure, loo_preds, data)

cv_results_pos <- loo_preds |>
  unnest(loo_preds) |> 
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

cv_results_cat <- loo_preds |>
  unnest(c(loo_preds, data), names_repair = "unique") |>
  group_by(language, measure, category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

cv_results_lex <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

eng_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "English (American)", measure == "produces") |>
  arrange(desc(abs_dev)) |>
  head(50)

```


```{r coefs}
lexcats <- set_names(
  paste0("lexical_category", 1:3),
  rownames(contrasts(aoa_models$group_data[[1]]$lexical_category))[1:3]
)
aoa_coefs <- aoa_models |>
  select(language, measure, coefs) |>
  unnest(coefs) |>
  filter(term != "(Intercept)") |>
  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
         effect = if_else(str_detect(term, ":"), "interaction", "main"),
         lexical_category = if_else(effect == "interaction",
                                    str_extract(term, "lexical_category.*:|:lexical_category.*"), if_else(grepl("lexical_category",term), term, as.character(NA))),
         lexical_category = str_remove(lexical_category, "lexical_category"),
         lexical_category = str_remove(lexical_category, ":"),
         lexical_category = fct_recode(lexical_category, !!!lexcats),
         term = if_else(effect == "interaction",
                        str_remove(term, "lexical_category.*:|:lexical_category.*"),
                        term),
         #term = fct_recode(term, !!!lexcats),
         #term = factor(term, levels = c(predictors, names(lexcats))),
         language = factor(language, levels = target_langs))

saveRDS(aoa_coefs, "data/aoa_coefs.rds" )
saveRDS(cv_results, "data/cv_results.rds" )
saveRDS(cv_results_pos, "data/cv_results_pos.rds" )
saveRDS(cv_results_cat, "data/cv_results_cat.rds" )
saveRDS(cv_results_lex, "data/cv_results_lex.rds" )
saveRDS(eng_across_lang_lex_desc, "data/eng_across_lang_lex_desc.rds" )

```


