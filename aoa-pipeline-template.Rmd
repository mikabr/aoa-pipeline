---
title: "AoA prediction template"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)
# load libraries
library(tidyverse)
library(modelr)
library(glue)
library(wordbankr)
library(childesr)
# load functions
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Load Wordbank data

Loading cached Wordbank data for English:
```{r load_wordbank_eng}
eng_wb_data <- load_wb_data("English (American)")
eng_wb_data
```

Defining languages:
```{r define_langs}
target_langs<-c("Spanish (Mexican)", "Spanish (European)",  "German",  "English (American)","Italian", "Norwegian", "Turkish", "Swedish", "Danish", "Dutch","French (French)","French (Quebecois)", "Portuguese (European)","Hungarian")
#"Croatian" no morphology
# Russian : transliteration of morphology
sinotibetan_languages <- c("Mandarin (Taiwanese)", "Mandarin (Beijing)")
```


Loading Wordbank data for multiple languages (cached or not):
```{r load_wordbank_xling}
wb_data <- load_wb_data(target_langs)
aoas <- fit_aoas(wb_data)
```

Creating saved Wordbank data for a language, for example:
```{r create_wordbank_eng, eval=FALSE}
eng_wb_data <- create_wb_data("English (American)")
```

Creating saved Wordbank data one step at a time (potentially making changes between steps), for example:
```{r create_wordbank_eng_steps, eval=FALSE}
eng_wg <- create_inst_data("English (American)", "WG")
eng_ws <- create_inst_data("English (American)", "WS")
eng_wg_summary <- collapse_inst_data(eng_wg)
eng_ws_summary <- collapse_inst_data(eng_ws)
eng_comb_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
```


# Load predictors

## Ratings and phonemes

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r merge_unilemmas}
uni_lemmas <- map_df(target_langs, extract_uni_lemmas, wb_data)
```

```{r load_predictors}
babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas |> map_predictor("babiness", babiness_map)
valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas |> map_predictor("valence", valence_map)
concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```

## CHILDES

Loading cached CHILDES metrics for English:
```{r load_childes_eng, eval=FALSE}
eng_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```



Creating saved CHILDES metrics for English, potentially changing which metrics are computed and/or arguments that are passed to `childesr` functions:
```{r specify_childes}
metric_funs <- list(compute_count, compute_mlu, compute_positions, compute_length_char, compute_length_phon,  compute_n_sfx_cat, compute_n_type, compute_verb_frame ) 
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")
```

Creating saved CHILDES data for many languages:
```{r create_childes_english, eval=FALSE}
eng_childes <- get_token_metrics("English (American)", metric_funs, corpus_args)
eng_unilemmas <- get_uni_lemma_metrics("English (American)", build_uni_lemma_map(uni_lemmas))
childes_metrics = eng_unilemmas
```


Creating saved CHILDES data for many languages:
```{r create_childes_xling, eval=TRUE}
walk(target_langs, get_token_metrics, metric_funs, corpus_args)
```

Creating saved UNILEMMA data for many languages:
```{r create_unilemma_xling, eval=TRUE}
walk(target_langs, get_uni_lemma_metrics, build_uni_lemma_map(uni_lemmas))
```

Loading cached CHILDES data for multiple languages:
```{r load_unilemma_xling, eval=TRUE}
childes_metrics <- load_childes_metrics(target_langs, uni_lemmas) |> filter(!is.na(uni_lemma))
```


Get phonology via eSpeak for tokens that didn't get it from CHILDES:
```{r eval=FALSE}
#Cannot run this, "no method" error
phonemes <- uni_lemmas |> map_phonemes()
childes_metrics <- childes_metrics |>
  left_join(phonemes, by = c("language", "uni_lemma")) |>
  mutate(length_char = coalesce(length_char, num_chars),
         length_phon = coalesce(length_phon, num_phons)) |>
  select(-c(num_chars, num_phons))
```


# Prepare data for modeling

## Frequency transformations

By default, `transform_counts()` transforms any column that starts with "count" by smoothing (add 1), normalizing, and log transforming, then renaming every column "count_x" to "freq_x". `residualize_freqs()` residualizes all columns that starts with "freq_" from the column "freq".

```{r prepare_frequency}
childes_metrics <- childes_metrics |> transform_counts() |> residualize_freqs() 
```

```{r prepare_morphology}
childes_metrics <- map_df(target_langs, residualize_morph, childes_metrics)
```

## Combining sources

Combine mapped predictors and CHILDES predictors:

```{r merge_all}
predictor_data_list <- list(babiness, concreteness, childes_metrics)
predictor_data <- predictor_data_list |>
  reduce(partial(full_join, by = c("language", "uni_lemma"))) 
```

## Setting predictors

```{r set_predictors}
predictor_sources <- list(
  c("freq"),
  c("concreteness","babiness"),
  c("length_char"), #"length_phon"
  c("n_type",  "n_cat", "n_affix"), 
  c("per_frame"),
  c("mlu"))
predictors<-unlist(predictor_sources)
```


## Preparing data for regression

```{r prep_data}
ref_cat ="nouns"
predictor_data_lexcat <- prep_lexcat(predictor_data, uni_lemmas, ref_cat) |>
  select(-c(item_definition)) |>
  distinct()
```



```{r prep_data2}
max_steps = 20
predictor_data_imputed_scaled <- do_full_imputation(predictor_data_lexcat, predictor_sources, max_steps)
```

 
## Merge with AOAs

```{r merge_aoa}
aoa_predictor_data <- predictor_data_imputed_scaled |> left_join(aoas) |>## merge with AOAs
  filter(!(aoa>36))
 
```

## Run model 

```{r}
run_main_model<-function(lang, aoa_predictor_data, predictors, uni_lemmas, ref_cat, lexcat_interactions = TRUE){
  print(lang)
  df = aoa_predictor_data|>filter(language ==lang)
  
  if (!(is.data.frame(df) && nrow(df)==0)){
  if (all(is.na(df[,"n_type"]))) {    
    predictors = predictors[predictors!= "n_type"]
  }
  if (all(is.na(df[,"n_affix"]))) {    
    predictors = predictors[predictors!= "n_affix"]
  }
  if (all(is.na(df[,"n_cat"]))) {    
    predictors = predictors[predictors!= "n_cat"]
  }
    if (all(is.na(df[,"per_frame"]))) {    
    predictors = predictors[predictors!= "per_frame"]
  }
  if (lang %in% sinotibetan_languages){
    predictors = predictors[predictors!= "length_char"]
  }  
  aoa_predictor_data <- aoa_predictor_data|> select(-lexical_category)|>
    filter(language==lang)
   aoa_predictor_data_lexcat  <- prep_lexcat(aoa_predictor_data, uni_lemmas |> filter(language==lang), ref_cat) ## Set again lexical category contrasts
   m <- fit_models(predictors, aoa_predictor_data_lexcat, lexcat_interactions) |> 
    mutate(language=lang)
  
   return(m)
  }
  else{
    return(NULL)
    }
}
aoa_models <- map_df(target_langs, run_main_model, aoa_predictor_data, predictors, uni_lemmas, ref_cat) |>unnest(coefs)
target_langs <- unique(aoa_models$language)  
#Some languages may not have enough resources to fit models, remove by target_langs
```

## Add per_frame feature only for verbs
```{r}
predictor_sources_verb <- list(
  c("freq"),
  c("concreteness","babiness"),
  c("length_char"), #"length_phon"
  c("n_type",  "n_cat", "n_affix"), 
  c("per_frame"),
  c("mlu"))
predictors_verb<-unlist(predictor_sources_verb)
aoa_models_verb <- map_df(target_langs, run_main_model, aoa_predictor_data|>filter(lexical_category =="predicates"), predictors_verb, uni_lemmas, ref_cat, lexcat_interactions = FALSE)
aoa_models_verb <- aoa_models_verb |> unnest(coefs) |>filter(term =="per_frame") 
common <- intersect(colnames(aoa_models), colnames(aoa_models_verb))
to_add <- aoa_models |>
  select(language, measure, group_data, stats, vifs)
#row-bind only on common column names
if (!(is.data.frame(aoa_models_verb) && nrow(aoa_models_verb)==0)){
  aoa_models <- rbind(aoa_models[common], aoa_models_verb[common])|>
    select(-c(group_data,stats, vifs )) |>
    nest(coefs =c(term, estimate, std.error, statistic, p.value)) |>
    left_join(to_add)
  } else {
  aoa_models <- aoa_models|>
  nest(coefs =c(term, estimate, std.error, statistic, p.value))
}  
```

## Model outputs

Coefficients:
```{r}
aoa_models |> select(language, measure, coefs) |> unnest(coefs) 
```

Summary stats:
```{r}
aoa_models |> select(language, measure, stats) |> unnest(stats)
```

Variance inflation factors:
```{r, eval = FALSE}
aoa_models |> select(language, measure, vifs) |> unnest(vifs)
```


## Cross-validation

```{r cross_validate}
run_cross_validation_model<-function(lang, meas, aoa_predictor_data, predictors, uni_lemmas, ref_cat){
  l = normalize_language(lang)
  aoa_predictor_data <- aoa_predictor_data|> select(-lexical_category)|>
    filter(language==lang, measure==meas)
  
  aoa_predictor_data_lexcat <- prep_lexcat(aoa_predictor_data, uni_lemmas |> filter(language==lang), ref_cat) ## Set again lexical category contrasts
  df = aoa_predictor_data|>filter(language ==lang)
  if (all(is.na(df[,"n_type"]))) {    
    predictors = predictors[predictors!= "n_type"]
  }
  if (all(is.na(df[,"n_affix"]))) {    
    predictors = predictors[predictors!= "n_affix"]
  }
  if (all(is.na(df[,"n_cat"]))) {    
    predictors = predictors[predictors!= "n_cat"]
  }
    if (all(is.na(df[,"per_frame"]))) {    
    predictors = predictors[predictors!= "per_frame"]
  }
  if (lang %in% sinotibetan_languages){
    predictors = predictors[predictors!= "length_char"]
  }  
loo_df <- aoa_predictor_data_lexcat |>
  group_nest(language, measure) |>
  mutate(language=lang,
         measure=meas,
    loo_models = ifelse(language %in% sinotibetan_languages, map(data, fit_cv_models, list(make_predictor_formula(predictors))), map(data, fit_cv_models, list(make_predictor_formula(predictors)))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))  
return(loo_df)
}
```

```{r implement_xval}
# for simple cv result extraction
xval_und_model <- map_df(unique((aoa_predictor_data %>% filter(measure=="understands"))$language),run_cross_validation_model,"understands", aoa_predictor_data, predictors, uni_lemmas, ref_cat) 
xval_prod_model <- map_df(unique((aoa_predictor_data %>% filter(measure=="produces"))$language),run_cross_validation_model,"produces", aoa_predictor_data, predictors, uni_lemmas, ref_cat) 
loo_df <- bind_rows(xval_und_model, xval_prod_model)
```


```{r extract_cross_validated_results}
cv_results <- loo_df |>
  select(language, measure, cv_results) |>
  unnest(cv_results)
loo_preds <- loo_df |>
  select(language, measure, loo_preds, data)
cv_results_pos <- loo_preds |>
  unnest(loo_preds) |> 
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))
cv_results_cat <- loo_preds |>
  unnest(c(loo_preds, data), names_repair = ~ make.names(., unique = TRUE)) |>
  group_by(language, measure, category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))
cv_results_lex <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))
eng_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "English (American)", measure == "produces") |>
  arrange(desc(abs_dev)) |>
  head(50)
man_t_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "Spanish (Mexican)", measure == "understands") |>
  arrange(desc(abs_dev)) |>
  head(50)
```


```{r extract_coefs}
lexcats <- set_names(
  paste0("lexical_category", 1:3),
  rownames(contrasts(aoa_models$group_data[[1]]$lexical_category))[1:3]
)
aoa_coefs <- aoa_models |>
  select(language, measure, coefs) |>
  unnest(coefs) |>
  filter(term != "(Intercept)") |>
  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
         effect = if_else(str_detect(term, ":"), "interaction", "main"),
         lexical_category = if_else(effect == "interaction",
                                    str_extract(term, "lexical_category[0-9]"),
                                    as.character(NA)),
         lexical_category = fct_recode(lexical_category, !!!lexcats),
         term = if_else(effect == "interaction",
                        str_remove(term, ":?lexical_category[0-9]:?"),
                        term),
         term = fct_recode(term, !!!lexcats),
         term = factor(term), #, levels = c(predictors, names(lexcats))
         language = factor(language, levels = target_langs))
```


```{r savedata, eval = TRUE }
saveRDS(predictor_data_lexcat,"data/plots/predictor_data_lexcat.rds")
saveRDS(aoa_predictor_data,"data/plots/aoa_predictor_data.rds" )
saveRDS(aoa_coefs, "data/plots/aoa_coefs.rds" )
saveRDS(cv_results, "data/plots/cv_results.rds" )
saveRDS(cv_results_pos, "data/plots/cv_results_pos.rds" )
saveRDS(cv_results_cat, "data/plots/cv_results_cat.rds" )
saveRDS(cv_results_lex, "data/plots/cv_results_lex.rds" )
saveRDS(eng_across_lang_lex_desc, "data/plots/eng_across_lang_lex_desc.rds" )
```


```{r save_worst_produced_data, eval = TRUE }
worst_predicted_unilemmas<- function(lang, meas, loo_preds){ loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == lang, measure == meas) |>
  arrange(desc(abs_dev)) |>
  head(50)
}
worst_uni_produce<-map_df(target_langs, worst_predicted_unilemmas, "produces", loo_preds) %>% 
  group_by(test_word) %>%
  summarise(count_lang=n(),
  lang_names = paste0(language, collapse = ","),
  aoa_mean=mean(aoa),
  aoa_pred_mean=mean(aoa_pred))
worst_uni_produce1<-loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(measure == "produces") |>
  filter(abs_dev>5)|>
  arrange(desc(abs_dev))
saveRDS(worst_uni_produce, "data/plots/worst_uni_produce.rds" )
```
