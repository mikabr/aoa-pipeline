---
title: "Aoa_prediction_reliability"
output: html_document
---

### Script to measure reliability of CHILDES predictors, WordBank CDIs and regression models for target languages. 

Load libraries, define target languages, predictors and paths.
```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

library(tidyverse)
library(glue)
library(wordbankr)
#library(broom)
#install.packages("remotes")
#remotes::install_github("langcog/childesr")
library(childesr)
#library(modelr)


walk(list.files("/Users/loukatou/Documents/aoa-pipeline/scripts", pattern = "*.R", full.names = TRUE), source)

target_langs <- c("French (French)", "Turkish", "Italian")#, "Italian", "Turkish"
#target_langs <- c("Danish", "Norwegian", "Croatian", "Italian", "Spanish (Mexican)", #"Turkish", "Swedish", "French (Quebecois)", "Russian") #, "English (American)"

#target_langs <- "artificial_simple"

childes_path <- "/Users/loukatou/Documents/aoa-pipeline/data/childes"

temp_path <- "/Users/loukatou/Documents/aoa-pipeline/data/temp_saved_data"

metric_funs <- list(compute_count, compute_mlu, compute_positions, compute_length_char, compute_length_phon,  compute_n_sfx_cat, compute_n_type, compute_verb_frame ) # 

corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")

```


### Functions CDI

* sbformula: adjusts correlation using the spearman-brown formula. https://assess.com/2018/04/14/what-is-the-spearman-brown-prediction-formula/

* measure_aoas: gets AoAs for CDI administrations using the get_aoas function

* split_wordbank: extracts WG and/or WS data from WordBank, and splits each form in random half. Then collapses forms.  

* get_main_rel_aoa: main wrapper function for CDI reliability, measures pearson correlation

```{r functions cdi}

# adjust with spearman-brown formula
sbformula <- function(r){
  if (!is.na(r)){
  r1<-(2*r)/(1+r)
  }else{
  r1=NA
  }
  return(r1)
}

#######################  


# measures aoa for each item
measure_aoas <- function(admin_summary) {
  d<- admin_summary |>
  mutate(num_false = total - num_true,
         prop = num_true / total) |>
  dplyr::select(language, measure, uni_lemma, prop, num_true,age, num_false, total, items) |>
         unique()
  d |>
  group_by(language, measure) |>
  nest() #
  admin_aoas <- fit_aoas(d) #|>
 # mutate(aoa = purrr::map(a$aoa)) |>  
  #mutate(aoa = purrr::map(d, fit_aoas)) |>
  #dplyr::select(-data) |>
  #unnest(cols = c(aoa))
  return(admin_aoas)
}
################################



# get wordbank and randomly split in two
split_wordbank <- function(lang, form, meas){
  f <- file.path(childes_path, glue("{lang}_{form}.rds"))
  if(!file.exists(f)) {
  print(glue("Extract {lang} {form} data from WordBank"))
    items <- tryCatch({items <- create_inst_data(lang, form) %>%
      filter(measure==meas)
      }, error= function(e){})
    } else {
    print(glue("Load saved {lang} {form} data"))
    items <-readRDS(f)
  }
  if (length(unique(items$data_id)) >1){ #if at least 1 administration
   admin<-as.data.frame(unique(items$data_id))
   ind <- sample(c(TRUE, FALSE), nrow(admin), replace=TRUE, prob=c(0.5, 0.5))
   print(glue("Randomly split administrations..."))
   adminfirstnum <- admin[ind, ]
   adminfirst<-items %>% filter(data_id %in% adminfirstnum)
   adminfirst_summary <- collapse_inst_data(adminfirst)
   
   adminsecondnum <- admin[!ind, ] #create two groups of administrations
   adminsecond<-items %>% filter(data_id %in% adminsecondnum)
   adminsecond_summary <- collapse_inst_data(adminsecond)
   
   df <- list(adminfirst_summary, adminsecond_summary)
  } else{
   df<- NULL
  }
return(df)
}

#################  


# main function for reliabilities of aoa within wordbank
get_main_rel_aoa <-function(lang, meas){
  langn = str_replace(lang, " ", "_")
  rel_aoa <- file.path(glue("/Users/loukatou/Documents/aoa-pipeline/data/reliabilities_aoa_{langn}.rds"))
  if(!file.exists(rel_aoa)){
  print(glue("Get CDI item data for {lang} {meas} and measure aoas..."))
  if (is.null(nrow(split_wordbank(lang, "WS", meas)[[1]]))){
     if (!is.null(nrow(split_wordbank(lang, "WG", meas)[[1]]))){
        first_aoas <- measure_aoas(split_wordbank(lang, "WG", meas)[[1]])
        second_aoas <-measure_aoas(split_wordbank(lang, "WG", meas)[[2]])
        r=cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson")
     }else{
    r=NA
     }
  }
  else if (is.null(nrow(split_wordbank(lang, "WG", meas)[[1]]))){
     if (!is.null(nrow(split_wordbank(lang, "WS", meas)[[1]]))){
    first_aoas <- measure_aoas(split_wordbank(lang, "WS", meas)[[1]])
    second_aoas <-measure_aoas(split_wordbank(lang, "WS", meas)[[2]])
    r=cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson")
     } 
  }
  else{   
  first_summary <- combine_form_data(list(split_wordbank(lang, "WS", meas)[[1]], split_wordbank(lang, "WG", meas)[[1]]))
  second_summary <- combine_form_data(list(split_wordbank(lang, "WS", meas)[[2]], split_wordbank(lang, "WG", meas)[[2]]))
  first_aoas <- measure_aoas(first_summary)
  second_aoas <-measure_aoas(second_summary)
  print(glue("Measure correlation ..."))
  r<-cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson") 
  }
  return(r)
  } else {
  r<- readRDS(rel_aoa) 
  r <- r|> filter(language == lang, measure == meas) |> select(r_aoa)
  return(r$r_aoa)
  }
}

```



### Measure reliabilities for aoa WordBank

Implement functions above, for each target language, measure (and optionally word_class). The mean correlation r is selected from 5 random splits of the administrations. 

```{r aoas, warning = FALSE, message=FALSE}

target_langs <- c("French (French)", "French (Quebecois)", "Spanish (Mexican)", "Spanish (European)",  "German", "Portuguese (European)", "Swedish", "Italian", "Norwegian", "Turkish", "Danish") #Dutch, Hungarian, Spanish (European)

#rel_aoa <- file.path(glue("/Users/loukatou/Documents/aoa-pipeline/data/reliabilities_aoa.rds")) #check whether cached data exist
#  if(!file.exists(rel_aoa)) {
    
reliabilities_aoa <- expand_grid(lang = target_langs,
                                 meas = c("produces", "understands"),
                            word_class = c("all")) %>% # 
  rowwise %>%
  mutate(r_aoa = ifelse(is.na(get_main_rel_aoa(lang, meas)), NA,
                        get_main_rel_aoa(lang, meas)),
  #(sum(get_main_rel_aoa(lang, meas), get_main_rel_aoa(lang, meas), get_main_rel_aoa(lang, meas),get_main_rel_aoa(lang, meas),get_main_rel_aoa(lang, meas)))/5),
          sbr_aoa = sbformula(r_aoa))
  #mutate(split_half_aoa = ifelse(word_class == "all", 
  #                           split_half_cor_aoa(language, measure),
  #                           split_half_cor_aoa(language, word_class)),
        
#  }else{
#reliabilities_aoa <-  readRDS(rel_aoa)
#}


pathrelaoa<- glue("data/reliabilities_aoa.rds")
saveRDS(reliabilities_aoa, pathrelaoa)
reliabilities_aoa %>%
  knitr::kable(digits = 2)

```
```{r functions childes}

target_langs <- c("French (French)", "French (Quebecois)", "Spanish (Mexican)", "Spanish (European)",  "German", "Portuguese (European)", "Swedish", "Italian", "Norwegian", "Turkish", "Danish") #Dutch, Hungarian, Spanish (European)

predictor_list <- c("length_char", "n_type", "mlu", "freq", "n_cat", "per_frame" ) #"n_affix"

# main function for reliabilities of predictors within childes
get_main_rel <-function(lang){
  langn = str_replace(lang, " ", "_")
  rel_childes <- file.path(glue("/Users/loukatou/Documents/aoa-pipeline/data/reliabilities_childes_{langn}.rds"))
  if(!file.exists(rel_childes)){
  both<-split_half(lang)
  df<- same_size_df(both[[1]],both[[2]])
  r=c(pred_cor("length_char", df),pred_cor("n_type", df), pred_cor("mlu", df),  pred_cor("freq", df), pred_cor("n_cat", df), pred_cor("per_frame", df)) # , pred_cor("n_affix", df),
  names= predictor_list
  rdata <- data.frame(names, r)
  rdata <- rdata %>% mutate(language=lang)
  }else{
  rdata <- readRDS(rel_childes)  
  }
  return(rdata)
}

######################

# add lemmas of one CHILDES half not existing at other half and do same size and order
same_size_df <- function(df1, df2) { 
  if (nrow(df1)>3){
  df1 <- df1 %>% 
    drop_na(freq)
  df2 <- df2 %>% 
    drop_na(freq)
  firstlistlemma<-(df1$uni_lemma)
    secondlistlemma<-(df2$uni_lemma)
    diff1<-setdiff(firstlistlemma,secondlistlemma) 
    diff2<-setdiff(secondlistlemma,firstlistlemma) 
    df<-as.data.frame(c(diff1, diff2)) %>%
      rename(uni_lemma = "c(diff1, diff2)" )
    df1<- anti_join(df1, df, by='uni_lemma')
    df2<- anti_join(df2, df, by='uni_lemma')
     both=list(tibble(df1[!duplicated(df1$uni_lemma),]), tibble(df2[!duplicated(df2$uni_lemma),]))
  return(both)
     }
     else{
    both=list(tibble(df1), tibble(df2))
    return(both)
  }
}

##########################

# extracts corr number from df
lang_pred_half <- function(lang, pred, split_corpora){
 s=split_corpora %>% filter(language==lang, names==pred)
 r=s[1,2]
return(as.numeric(r) ) 
}

#####################

# measure correlation between two vectors
pred_cor <- function(pred, df){
r<- cor(df[[1]][[pred]], df[[2]][[pred]], method="pearson", use="complete.obs")
return(r)  
}

#######################  

#get unilemmas for words, and predictors for unilemmas
get_predictors<- function(half, lang){
     token_metrics<- get_token_metrics(lang=lang, metric_funs, corpus_args, import_data=half)
     wg <- tryCatch({create_inst_data(lang, "WG")}, error = function(e){})
     ws <- tryCatch({create_inst_data(lang, "WS")}, error = function(e){})
     wg_summary <- tryCatch({collapse_inst_data(wg)}, error = function(e){})
     ws_summary <- tryCatch({collapse_inst_data(ws)}, error = function(e){})
     if (!is.null(wg) & !is.null(ws)){
      comb_summary <- combine_form_data(list(wg_summary, ws_summary))
     } else if (!is.null(wg)){
      comb_summary <- wg_summary 
     } else if (!is.null(ws)){
      comb_summary <- ws_summary 
     } else{
      comb_summary <- NULL 
     }
     if (!is.null(comb_summary)){
      uni_lemmas <- extract_uni_lemmas(comb_summary)
      uni_metrics<- get_uni_lemma_metrics(lang, build_uni_lemma_map(uni_lemmas), token_metrics)
      uni_metrics_ <- uni_metrics |> transform_counts() |> residualize_freqs() |>
  residualize_morph_s() 
      #uni_metrics <- prepare_frequency(lang, uni_metrics, uni_lemmas, count, count_first, count_last, count_solo, frequency, first_frequency, final_frequency, solo_frequency) 
     return(uni_metrics_[order(uni_metrics$uni_lemma),])
     } else{
      wb_data <- load_wb_data(lang)
      uni_lemmas <- extract_uni_lemmas(wb_data)
      uni_metrics<- get_uni_lemma_metrics(lang, build_uni_lemma_map(uni_lemmas), token_metrics)
      uni_metrics_ <- uni_metrics |> transform_counts() |> residualize_freqs() |>
  residualize_morph_s() 
     return(uni_metrics_[order(uni_metrics$uni_lemma),])  
     }
}
#############################

#get CHILDES and randomly splits words in half
split_half <-function(lang){ 
  childes_lang <- convert_lang_childes(lang)
  file_t <- file.path(childes_path, glue("tokens_{childes_lang}.rds"))
  if(file.exists(file_t)) {
   tokens <-readRDS(file_t)
  }else{
   tokens <- get_childes_data(childes_lang, corpus_args)[[2]] 
  }
  tokens <- tokens %>% mutate(gloss=tolower(gloss))
  file_u <- file.path(childes_path, glue("utterances_{childes_lang}.rds"))
  if(file.exists(file_u)) {
   utterances <- readRDS(file_u)
  }else{
   utterances <- get_childes_data(childes_lang, corpus_args)[[1]] 
  } 
 
  ind <- sample(c(TRUE, FALSE), nrow(tokens), replace=TRUE, prob=c(0.5, 0.5)) #split tokens
  tokens1 <- tokens[ind, ] #split in two
  tokens2 <- tokens[!ind, ] 
  
   utterances1 <- utterances %>%
     filter(id %in% tokens1$utterance_id) %>%
     mutate(gloss=tolower(gloss))  

    utterances2 <- utterances %>%
     filter(id %in% tokens2$utterance_id) %>%
     mutate(gloss=tolower(gloss)) 
    
  half1 <- list("utterances" = utterances1, "tokens" = tokens1)
  half2 <- list("utterances" = utterances2, "tokens" = tokens2)

  both=list(tibble(get_predictors(half1, lang)), tibble(get_predictors(half2, lang)))
  return(both)  
  }

```



### Measure reliabilities for CHILDES predictors

Implement functiosn above, for each target language, measure (and optionally word_class). The mean correlation r is selected from 5 random splits of the corpus. 

```{r childes, warning = FALSE, message=FALSE}

rel <- file.path(glue("/Users/loukatou/Documents/aoa-pipeline/data/reliabilities.rds")) #check whether cached data exist
  if(!file.exists(rel)) {
 
split_corpora <- lapply(target_langs, get_main_rel) %>% bind_rows()

#split_corpora1 <- lapply(target_langs, get_main_rel) %>% bind_rows()

#split_corpora2 <- lapply(target_langs, get_main_rel) %>% bind_rows()

#split_corpora3 <- lapply(target_langs, get_main_rel) %>% bind_rows()

#split_corpora4 <- lapply(target_langs, get_main_rel) %>% bind_rows()

predictor_list <- c("length_char", "n_type", "mlu", "freq", "n_cat", "per_frame", "n_affix" ) #


reliabilities <- expand_grid(lang = target_langs,
                            word_class = c("all"),
                            pred = predictor_list) %>%  
                            rowwise %>%        
                            mutate(r = lang_pred_half(lang, pred, split_corpora)) |>
                        #    mutate(r =  (sum(lang_pred_half(lang, pred, split_corpora), lang_pred_half(lang, pred, split_corpora1), lang_pred_half(lang, pred, split_corpora2), lang_pred_half(lang, pred, split_corpora3), lang_pred_half(lang, pred, split_corpora4)))/5) %>% 
  mutate(sbr = sbformula(as.numeric(r)))

reliabilities %>%
  knitr::kable(digits = 2)

splitpath<- glue("data/reliabilities.rds")
saveRDS(reliabilities, splitpath)
  }else{
reliabilities <-  readRDS(rel)
}


```

### Measure R squared from cross-validation 

The rsq_cv function measures the R squared of the cross-validated test models
```{r x-validation}
sinotibetan_languages <- c("Mandarin (Taiwanese)", "Mandarin (Beijing)")

rsq <- file.path(glue("/Users/loukatou/Documents/aoa-pipeline/data/rsq.rds")) #check whether cached data exist
  if(!file.exists(rsq)) {

#Italian: 25, 308 (bath, sit, a)
rsq_cv<- function(word_values, predictor, lang, meas) {
  r <- word_values |>
    filter(language == lang, measure == meas) |>
  group_nest(language, meas) |>
  mutate(#language=lang,
         #measure=meas,
    loo_models = ifelse(language %in% sinotibetan_languages, map(data, fit_cv_models, list(make_predictor_formula(predictor))), map(data, fit_cv_models, list(make_predictor_formula(predictor)))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))  

  #r<-word_values |>
  #filter(language==lang, measure==meas) |>
  #group_nest() |>
  #mutate(loo_data = map(data, crossv_loo),
  #       loo_models = map(data, fit_cv_models, list(make_predictor_formula(predictor, TRUE))),
  #       loo_preds = map2(loo_models, data, get_cv_preds),
  #       cv_results = map(loo_preds, get_cv_results))
  r_lp<- r |>
    unnest(cv_results) %>% 
    dplyr::select(r2) %>%
   unique()
  r=r_lp[1,1]
  return(as.character(r)) 
  }

#file_wv <- file.path(glue("/Users/lscpuser/Documents/aoa-pipeline8/aoa-pipeline-main-1/data/word_values.rds"))

file_wv <- file.path(glue("~/Documents/aoa-pipeline/data/aoa_predictor_data1.rds"))
word_values1 <- readRDS(file_wv) ####data needed from aoa-template

predictor_list <- c("length_char", "n_type", "mlu", "freq", "n_cat", "n_affix", "per_frame" )

word_values1 <- word_values1 %>% 
  filter(!(language=="Russian" & category=="sounds"))

target_langs1 <- c("French (French)", "French (Quebecois)", "Spanish (Mexican)", "Spanish (European)", "Swedish") #Dutch, Hungarian, Spanish (European) , "Italian", "Norwegian", "Turkish", "Danish"

target_langs2 <- c("Italian", "Norwegian", "Turkish", "Danish")

target_langs3 <- c("German", "Portuguese (European)") #Dutch, Hungarian, Spanish (European) , "Italian", "Norwegian", "Turkish", "Danish"

predictor_list2 <- c("length_char","n_type", "mlu", "freq", "n_cat","per_frame" )

file_wv2 <- file.path(glue("~/Documents/aoa-pipeline/data/aoa_predictor_data2.rds"))
word_values2 <- readRDS(file_wv2) ####data needed from aoa-template


rsq1 <- expand_grid(  #     class = c("all"),
                   pred = predictor_list,
                   lang = target_langs1,
                   meas = c("produces", "understands")) %>% 
                   rowwise %>%
                   mutate(r2= rsq_cv(word_values1, pred, lang, meas))

rsq2 <- expand_grid(  #     class = c("all"),
                   pred = predictor_list2,
                   lang = target_langs2,
                   meas = c("produces", "understands")) %>% 
                   rowwise %>%
                   mutate(r2= rsq_cv(word_values2, pred, lang, meas))

rsq3 <- expand_grid(  #     class = c("all"),
                   pred = predictor_list,
                   lang = target_langs3,
                   meas = c("produces")) %>% 
                   rowwise %>%
                   mutate(r2= rsq_cv(word_values1, pred, lang, meas))

rsq <- rbind(rsq2, rsq3)
rsq <- do.call("rbind", list(rsq1, rsq2, rsq3))

rsqpath<- glue("data/rsq.rds")
saveRDS(rsq, rsqpath)

}else{
rsq <-  readRDS(rsq)
}

```

### Merge to create final corpus 
```{r final corpus}

dfinal <- reliabilities %>%
  left_join(reliabilities_aoa) %>%
  unique() %>%
  filter(!is.na(meas)) %>% 
  mutate(threshold = sbr * sbr_aoa) %>%
  left_join(unique(rsq)) %>%
 # filter(!is.infinite(as.numeric(r2))) %>%
  unique()


dfinalpath<- glue("data/dfinal.rds")
saveRDS(dfinal, dfinalpath)

dfinal %>%
  knitr::kable(digits = 2)
```


######
```{r plot}
relia_plot <- function(dfinal, lang_list){
 ggplot() +  
  geom_errorbar(dfinal, mapping=aes(x=pred, y=threshold, ymax=threshold, ymin=threshold,  color=pred))+
  geom_bar(dfinal,mapping=aes(x=pred, y=as.numeric(r2), fill=pred), position="dodge", stat="identity")  +
  facet_grid(lang ~ meas) +
  theme(legend.position = "bottom") +
  ylab("Predictor") +
  xlab("R2") +
  #ylim(-1,1)+  
  theme(legend.title = element_blank())+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30))
}
relia_plot(dfinal, target_langs)
```




