---
title: "Aoa_prediction_reliability_morphosyntax"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

library(tidyverse)
library(glue)
library(wordbankr)
#library(childesr)

normalize_language <- function(language) {
  language %>% str_replace(" ", "_") %>% str_to_lower()
}

walk(list.files("scripts", pattern = "*.R", full.names = TRUE), source)

#target_langs <- c("Russian", "Turkish", "Croatian", "Danish", "Italian")
target_langs <- c("Italian")

childes_path <- "data/childes/"

```


### Load saved CHILDES .csv corpus for a language (here French and Italian). 

Rbind different corpora.

```{r cdi}

load_cdi<- function(lang) {
  words <- get_inst_words(lang, "WS")
  admins <- get_inst_admins(lang, "WS")
  items <- get_instrument_data(language = lang,
                                 items = words$item_id,
                                 form = "WS",
                                 administrations = admins,
                                 iteminfo = words) %>%
                                 rename(lemma = definition) 
  return(items)
}
```

```{r aoas}
#aoas <- map_df(target_langs, function(lang) {
#  items <- load_cdi(lang)
#  aoa <- fit_aoa(items, measure = "produces", method = "glmrob", proportion = 0.5)%>%
#                                 filter(!is.na(aoa), !is.na(uni_lemma)) %>%
#                                  mutate(language=lang)
#  return(aoa)
#})
wb_data <- map_df(target_langs, function(lang) {
  message(glue("Loading data for {lang}..."))
  norm_lang <- normalize_language(lang)
  readRDS(glue("data/wordbank/{norm_lang}.rds"))
})

wb_data <- wb_data |>
  mutate(num_false = total - num_true,
         prop = num_true / total) |>
  select(language, measure, uni_lemma, age, prop, num_true, num_false, total, items)

aoas <- wb_data |>
  group_by(language, measure) |>
  nest() |>
  mutate(aoa = map(data, get_aoas, max_steps = 400)) |>
  select(-data) |>
  unnest(cols = c(aoa))

uni_lemmas <- get_uni_lemmas(wb_data) %>%
  unnest(items) %>%
  left_join(aoas) %>%
  filter(!is.na(aoa), !is.na(uni_lemma)) %>%
  nest(items = c(category, definition)) %>%
  unique()
  
```

```{r predictors}

predictors <- map_dfr(target_langs, function(lang) {
  norm_lang <- normalize_language(lang)
  childes_lang <- convert_lang_childes(lang)
  file_ <- file.path(childes_path, glue("uni_metrics_{norm_lang}.rds"))
  childes_predictors <- readRDS(file_)|>
    group_by(language, uni_lemma)}) 

predictors <- normalize_frequency(predictors)  

```

### Plot frequency and aoa using log frequency and model intercept frequency
```{r plot}
#ggplot(d, 
#       aes(x=freq, y=aoa, label=uni_lemma)) + 
#    geom_smooth(method = "lm", formula = y~x) + 
#    geom_point(alpha=.1) +
#    facet_grid(lexical_class~language) +
#  scale_x_log10() + 
#  xlab("Frequency (log10)") + 
#  ylab("Age of Acquisition (months)") + 
#  theme_bw()

```
### Reliability_frequency: half-split and Spearman-Brown

```{r reliability}

metric_funs <- list(compute_count, compute_mlu, compute_positions,
                    compute_length_char, compute_length_phon)
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")
uni_lemma_map = build_uni_lemma_map(uni_lemmas)

####################### 

# add lemmas of the first half not existing at the second half, with 1 
same_size_df <- function(df1, df2) { 
  firstlistlemma<-(df1$uni_lemma)
  secondlistlemma<-(df2$uni_lemma)
  diff1<-setdiff(firstlistlemma,secondlistlemma) 
  diff2<-setdiff(secondlistlemma,firstlistlemma) 
  df<-as.data.frame(c(diff1, diff2))
  colnames(df)<- c("uni_lemma")
  add_missing <- function(df1, df){
      df1<- df1 %>%
      full_join(df) %>%
      arrange(uni_lemma)
      return(df1)
      }
    df1<- add_missing(df1, df)
    df2<- add_missing(df2, df)
  both=list(tibble(df1[!duplicated(df1$uni_lemma),]), tibble(df2[!duplicated(df2$uni_lemma),]))
  return(both)
}

#######################   

split_predictors<- function(tokens, childes_lang){
     utterances <- readRDS(glue(childes_path, "{childes_lang}_utterances.rds")) %>%
     filter(id %in% tokens$utterance_id)
     half <- list("utterances" = utterances, "tokens" = tokens)
     token_metrics<- get_childes_metrics(lang=lang, metric_funs, corpus_args, import_data=half)
     uni_metrics<- get_uni_lemma_metrics(lang=lang, uni_lemma_map, token_metrics)
     uni_metrics <- normalize_frequency(uni_metrics)  
    return(uni_metrics[order(uni_metrics$uni_lemma),])
  }

#######################  

split_half <-function(lang){ #word_class
  childes_lang <- convert_lang_childes(lang)
  tokens <-readRDS(glue(childes_path, "{childes_lang}_tokens.rds"))
  ind <- sample(c(TRUE, FALSE), nrow(tokens), replace=TRUE, prob=c(0.5, 0.5)) #split tokens
  tokens1 <- tokens[ind, ] #split in two
  tokens2 <- tokens[!ind, ] #split in two
  both=list(tibble(split_predictors(tokens1, childes_lang)), tibble(split_predictors(tokens2, childes_lang)))
  return(both)  
  }

#######################
sbformula <- function(r){  #adjust with spearman-brown formula
  r1<-(2*r)/(1+r)
  return(r1)
}

#######################  

corr_split<-function(lang){
both<-split_half(lang)
df<- same_size_df(both[[1]],both[[2]])
num_zero <- function(vector_with_nas) {
  vector_with_nas[is.na(vector_with_nas)] <- 0
  return(as.numeric(vector_with_nas))
}
length_char_r=cor(num_zero(df[[1]]$length_char), num_zero(df[[2]]$length_char), method="pearson")
n_tokens_r=cor(num_zero(df[[1]]$n_tokens), num_zero(df[[2]]$n_tokens), method="pearson")
mlu_r=cor(num_zero(df[[1]]$mlu), num_zero(df[[2]]$mlu), method="pearson")
frequency_r=cor(num_zero(df[[1]]$frequency), num_zero(df[[2]]$frequency), method="pearson")
raw_frequency_r=cor(num_zero(df[[1]]$freq_raw), num_zero(df[[2]]$freq_raw), method="pearson")
count_r=cor(num_zero(df[[1]]$count), num_zero(df[[2]]$count), method="pearson")

solo_frequency_r=cor(num_zero(df[[1]]$solo_frequency), num_zero(df[[2]]$solo_frequency), method="pearson")
first_frequency_r=cor(num_zero(df[[1]]$first_frequency), num_zero(df[[2]]$first_frequency), method="pearson")
final_frequency_r=cor(num_zero(df[[1]]$final_frequency), num_zero(df[[2]]$final_frequency), method="pearson")
r=c(length_char_r,n_tokens_r, mlu_r, count_r, raw_frequency_r, frequency_r, solo_frequency_r, first_frequency_r, final_frequency_r)
names= c("length_char","n_tokens", "mlu", "count", "raw_frequency","frequency", "solo_frequency", "first_frequency", "final_frequency")
rdata <- data.frame(names, r)
rdata <- rdata %>% mutate(language=lang)
return(rdata)
}



``` 

### Reliability_frequency: cronbach alpha


### Measure reliabilities

```{r apply_reliability_frequency}

lang_pred_half <- function(lang, pred, split_corpora){
 s=split_corpora %>% filter(language==lang, names==pred)
 r=s[1,2]
return(as.character(r) ) 
}


split_corpora <- lapply(target_langs, corr_split) %>%
  bind_rows() #####could not find croatianpy

reliabilities <- expand_grid(lang = c("Italian"),
                            word_class = c("all"),
                            pred = c("length_char","n_tokens", "mlu", "count", "raw_frequency","frequency", "solo_frequency", "first_frequency", "final_frequency")) %>%  #"nouns","adjectives","verbs","function_words","other
                rowwise %>%            
 mutate(corrr =  lang_pred_half(lang, pred, split_corpora)) %>% #split_half
  mutate(sbr = sbformula(as.numeric(corrr)))

reliabilities %>%
  knitr::kable(digits = 2)


```  

### Reliability_AoA

```{r reliability_aoa}  

split_half_cor_aoa <-function(lang){
  #print(glue("Get CDI item data for {lang_} and {clas_} ..."))
  items <-load_cdi(lang)
  admin<-as.data.frame(unique(items$data_id))
  
  print(glue("Randomly split administrations..."))
  ind <- sample(c(TRUE, FALSE), nrow(admin), replace=TRUE, prob=c(0.5, 0.5)) #randomly split administrations
  adminfirstnum <- admin[ind, ]
  adminsecondnum <- admin[!ind, ] #create two groups of administrations
  
  adminfirst<-items %>% filter(data_id %in% adminfirstnum) #filter items in administrations
  adminsecond<-items %>% filter(data_id %in% adminsecondnum)
  
  aoafirst<- fit_aoa(adminfirst, method = "glmrob", proportion = 0.5) # get aoa for each group
  aoasecond<- fit_aoa(adminsecond, method = "glmrob", proportion = 0.5) # 
  
  r<-cor(aoafirst$aoa, aoasecond$aoa, use="complete.obs", method="pearson") #measure r
  return(r)
}


```

```{r apply_reliability_aoa}

reliabilities_aoa <- expand_grid(language = c( "Italian"),
                            word_class = c("all")) %>% # "nouns","adjectives","verbs",   "function_words","other"
  rowwise %>%
  mutate(split_half_aoa = ifelse(word_class == "all", 
                             split_half_cor_aoa(language),
                             split_half_cor_aoa(language, word_class)),
         split_half_aoa_sb = sbformula(split_half_aoa))

reliabilities_aoa %>%
  knitr::kable(digits = 2)


```

### Regression

```{r final_Data}
build_final_data <- function(lang){
predictor_data <- predictors %>% filter(language==lang)
aoas <- aoas %>% filter(language== lang)
word_values <- aoas |>
  left_join(predictor_data) # |>
              #select(-data) |>
              #unnest(cols = c(imputed)), 
          #  by = c("language", "uni_lemma"))
joined_data <- wb_data |>
  left_join(word_values, by = c("language", "measure", "uni_lemma"))%>%
  filter(!is.na(aoa), !is.na(uni_lemma), !is.na(count))  %>%
  unique()
return(joined_data)
}

#d <- aoas %>%
#  mutate(language=lang) %>%  
#  left_join(predictor_data) %>%
#  ungroup() %>% 
#  select(-c(item_id, num_item_id)) %>%
#  filter(!is.na(aoa), !is.na(uni_lemma), !is.na(count))  %>%
#  unique()


###aoas <- wb_data |>
##  group_by(language, measure) |>
##  nest() |>
##  mutate(aoa = map(data, get_aoas, max_steps = 400)) |>
##  select(-data) |>
##  unnest(cols = c(aoa))

```


```{r main_regression}

d <- lapply(target_langs, build_final_data) %>%
  bind_rows() #####could not find croatianpy


prepped_data <- d |>
  # select out just the by lexical item data
  unnest(cols = "items") |>
  # select(-c(age, num_true, total, form, item_id)) |>
  distinct() |>
  # pull out categories from classes
  mutate(lexical_category = if_else(str_detect(lexical_class, ","), "other",
                                    lexical_class),
         # collapse v, adj, adv into one category
         lexical_category = lexical_category |> as_factor() |>
           suppressWarnings(
             fct_collapse("predicates" = c("verbs", "adjectives", "adverbs"))
             )) |>
  select(-lexical_class)

#predictors <- pred_sources |> unlist()

#fitted_aoa_models <- fit_models(joined_data, predictors, full = FALSE)$rquared


 #regression_option1<-function(d,  pr){
 #predictor <- d[[pr]]
 #option1<-lm(aoa~predictor, d) 
 #return(summary(option1)$adj.r.squared)
 #}
# 
 rsq <- expand_grid(lang = c("Italian"), 
              #     class = c("all"),
                   predictor = c("length_char"),
                    measures= c("produces"))#,"n_tokens", "mlu", "count", "frequency", #"solo_frequency", "first_frequency", "final_frequency")) %>% 
  rowwise %>%
  mutate(r=fit_models(prepped_data %>% filter(language==lang, measure==measures), predictor, full=FALSE)) %>%
#  mutate(r2=r$rsquared[1]) 
  rename( language = lang)

```





```{r cross_validate_regression}

#divide into training and test set
#library(modelr)

#xvalr2 <- function(d, pr){
# n<-nrow(d) #df size
# ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.9, 0.1)) #randomly split lines
# train <- d[ind, ] 
# test <- d[!ind, ]
# predictor <- train[[pr]]
# model <- lm(aoa~ predictor, data=train) 
# predictor <- test[[pr]]
# predictions <- predict(model, test)
# crossvalr2 <- rsquare(model, test)
# return(crossvalr2)
#}

```


```{r final_data}  
dfinal <- reliabilities %>%
  mutate(word_class = ifelse(word_class == "", 
                            "all", word_class)) %>%
  rename(language=lang, predictor=pred) %>%
  left_join(reliabilities_aoa) %>%
    mutate(threshold_half = sbr * split_half_aoa_sb) %>%
      left_join(unique(r2))

dfinal %>%
  knitr::kable(digits = 2)
```


```{r final_plot_2}


ggplot(dfinal, aes(y = r2, x=predictor)) + 
   geom_bar(stat="identity") + 
  geom_point() + 
  facet_grid(cols = vars(word_class), rows = vars(language)) +
  geom_errorbar(data  = dfinal, aes(y=threshold_half, ymax=threshold_half, ymin=threshold_half)) + 
  theme(legend.position = "bottom") + 
  ylab("R2") + 
  xlab("Predictor") + 
  theme(legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 70))

```



```{r reliability_alpha}

# cronbach_alpha <-function(dataAoa, corpus_frequency_){ 
#  lang_str <- normalize_language(lang) 
#   corpus <- read_csv(glue("data/childes/childes_tokens_{lang_str}.csv"))
#   corpus1 <- corpus %>% 
#     filter(gloss %in% wblemmas) %>% 
#       group_by(gloss, target_child_id) %>% 
#          summarize(count=n())  
#   corpus2 <- corpus %>%  
#     group_by(target_child_id) %>% 
#      summarize(total=n()) 
#   corpus <- corpus1 %>% 
#    left_join(corpus2) %>% 
#       mutate(freq=count/total) 

# corpus_ <- corpus %>% 
#        mutate(stem = stem(gloss, convert_stemlang(lang))) %>% 
#         full_join(load_unilemmas() %>% filter(language == normalize_language(lang)),
# by = "stem") %>% -->
#           group_by(uni_lemma, target_child_id) %>% 
#             mutate(FreqLemma = sum(freq, na.rm = TRUE))#

#   lemma_<-corpus_$uni_lemma  #restructure dataframe 
#   target_child_id_<-corpus_$target_child_id 
#   freq_<-corpus_$FreqLemma 

#   df<-unique(data.frame(lemma_, target_child_id_, freq_))
#   corpus<-tidyr::spread(df, target_child_id_, freq_) 

#  child_ids_<- unique(as.character(colnames(corpus)[3:ncol(corpus)])) 
#  child<-select(corpus, child_ids_ ) 
#   a<-alpha(child) 
#   return(a$total[1,1])  


```

### Measure intercept frequency

```{r intercept frequency}

#source("measure_frequency.R") # add intercept function to aoa-pipeline
#frequencies <- lapply(corpus, frequency_model) %>%bind_rows()

#frequencies %>% 
 # group_by(target_child_id) %>% 
  #summarize (sum(rawFrequency)) #Test frequence: should be 1 for each child

# TEST2 frequency metric:   
#frequencies %>% 
 # arrange(desc(FrequencyLog))#: maximum values

```

```{r final_plot_1}
#ggplot(d, aes(x = freq, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("Frequency") + 
#  ylab("Age of production (months)")


#ggplot(d, aes(x = mlu, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("MLU") + 
#  ylab("Age of production (months)")

#ggplot(d, aes(x = solo_freq, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("Solo freq") + 
#  ylab("Age of production (months)")
```

