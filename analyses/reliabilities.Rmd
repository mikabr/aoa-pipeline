---
title: "Aoa_prediction_reliability"
output: html_document
---

### Script to measure reliability of CHILDES predictors, WordBank CDIs and regression models for target languages. 

Load libraries, define target languages, predictors and paths.
```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

library(tidyverse)
library(glue)
library(wordbankr)
#library(broom)
#install.packages("remotes")
#remotes::install_github("langcog/childesr")
library(childesr)
library(modelr)


walk(list.files("scripts", pattern = "*.R", full.names = TRUE), source)

target_langs <- c("Danish", "Norwegian", "Croatian", "English (American)", "Italian", "Spanish (Mexican)", "Turkish", "Swedish", "French (Quebecois)", "Russian")

predictor_list <- c("length_char","n_tokens", "mlu", "frequency", "solo_frequency", "first_frequency", "final_frequency")

childes_path <- "data/childes"

temp_path <- "data/temp_saved_data"

metric_funs <- list(compute_count, compute_mlu, compute_positions,
                    compute_length_char, compute_length_phon)
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")

```


### Functions CDI

* sbformula: adjusts correlation using the spearman-brown formula. https://assess.com/2018/04/14/what-is-the-spearman-brown-prediction-formula/

* measure_aoas: gets AoAs for CDI administrations using the get_aoas function

* split_wordbank: extracts WG and/or WS data from WordBank, and splits each form in random half. Then collapses forms.  

* get_main_rel_aoa: main wrapper function for CDI reliability, measures pearson correlation

```{r functions cdi}

# adjust with spearman-brown formula
sbformula <- function(r){
  if (!is.na(r)){
  r1<-(2*r)/(1+r)
  }else{
  r1=NA
  }
  return(r1)
}

#######################  


# measures aoa for each item
measure_aoas <- function(admin_summary) {
  d<- admin_summary |>
  mutate(num_false = total - num_true,
         prop = num_true / total) |>
  dplyr::select(language, measure, uni_lemma, prop, num_true,age, num_false, total, items) %>%
         unique()
  admin_aoas <- d|>
  group_by(language, measure) |>
  nest() |>
  mutate(aoa = map(data, get_aoas, max_steps = 400)) |>
  dplyr::select(-data) |>
  unnest(cols = c(aoa))
  return(admin_aoas)
}
################################



# get wordbank and randomly split in two
split_wordbank <- function(lang, form, meas){
  f <- file.path(temp_path, glue("{lang}_{form}.rds"))
  if(!file.exists(f)) {
  print(glue("Extract {lang} {form} data from WordBank"))
    items <- tryCatch({items <- create_inst_data(lang, form) %>%
      filter(measure==meas)
      }, error= function(e){})
    } else {
    print(glue("Load saved {lang} {form} data"))
    items <-readRDS(f)
  }
  if (length(unique(items$data_id)) >1){ #if at least 1 administration
   admin<-as.data.frame(unique(items$data_id))
   ind <- sample(c(TRUE, FALSE), nrow(admin), replace=TRUE, prob=c(0.5, 0.5))
   print(glue("Randomly split administrations..."))
   adminfirstnum <- admin[ind, ]
   adminfirst<-items %>% filter(data_id %in% adminfirstnum)
   adminfirst_summary <- collapse_inst_data(adminfirst)
   
   adminsecondnum <- admin[!ind, ] #create two groups of administrations
   adminsecond<-items %>% filter(data_id %in% adminsecondnum)
   adminsecond_summary <- collapse_inst_data(adminsecond)
   
   df <- list(adminfirst_summary, adminsecond_summary)
  } else{
   df<- NULL
  }
return(df)
}

#################  

# main function for reliabilities of aoa within wordbank
get_main_rel_aoa <-function(lang, meas){
  print(glue("Get CDI item data for {lang} {meas} and measure aoas..."))
  if (is.null(nrow(split_wordbank(lang, "WS", meas)[[1]]))){
     if (!is.null(nrow(split_wordbank(lang, "WG", meas)[[1]]))){
        first_aoas <- measure_aoas(split_wordbank(lang, "WG", meas)[[1]])
        second_aoas <-measure_aoas(split_wordbank(lang, "WG", meas)[[2]])
        r=cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson")
     }else{
    r=NA
     }
  }
  else if (is.null(nrow(split_wordbank(lang, "WG", meas)[[1]]))){
    first_aoas <- measure_aoas(split_wordbank(lang, "WS", meas)[[1]])
    second_aoas <-measure_aoas(split_wordbank(lang, "WS", meas)[[2]])
    r=cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson")
    } 
  else{   
  first_summary <- combine_form_data(list(split_wordbank(lang, "WS", meas)[[1]], split_wordbank(lang, "WG", meas)[[1]]))
  second_summary <- combine_form_data(list(split_wordbank(lang, "WS", meas)[[2]], split_wordbank(lang, "WG", meas)[[2]]))
  first_aoas <- measure_aoas(first_summary)
  second_aoas <-measure_aoas(second_summary)
  print(glue("Measure correlation ..."))
  r<-cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson") #measure r
    }
  return(r)
}

```



### Measure reliabilities for aoa WordBank

Implement functions above, for each target language, measure (and optionally word_class). The mean correlation r is selected from 5 random splits of the administrations. 

```{r aoas, warning = FALSE, message=FALSE}

rel_aoa <- file.path(glue("data/reliabilities_aoa.rds")) #check whether cached data exist
  if(!file.exists(rel_aoa)) {
    
reliabilities_aoa <- expand_grid(lang = target_langs,
                                 meas = c("produces", "understands"),
                            word_class = c("all")) %>% # 
  rowwise %>%
  mutate(r_aoa = ifelse(is.na(get_main_rel_aoa(lang, meas)), NA, (sum(get_main_rel_aoa(lang, meas), get_main_rel_aoa(lang, meas), get_main_rel_aoa(lang, meas),get_main_rel_aoa(lang, meas),get_main_rel_aoa(lang, meas)))/5),
          sbr_aoa = sbformula(r_aoa))
  #mutate(split_half_aoa = ifelse(word_class == "all", 
  #                           split_half_cor_aoa(language, measure),
  #                           split_half_cor_aoa(language, word_class)),
        
  }else{
reliabilities_aoa <-  readRDS(rel_aoa)
}


pathrelaoa<- glue("data/reliabilities_aoa.rds")
saveRDS(reliabilities_aoa, pathrelaoa)
reliabilities_aoa %>%
  knitr::kable(digits = 2)

```
```{r functions childes}

# main function for reliabilities of predictors within childes
get_main_rel <-function(lang){
  both<-split_half(lang)
  df<- same_size_df(both[[1]],both[[2]])
  r=c(pred_cor("length_char", df),pred_cor("n_tokens", df), pred_cor("mlu", df),  pred_cor("frequency", df), pred_cor("solo_frequency", df), pred_cor("first_frequency", df), pred_cor("final_frequency", df))
  names= predictor_list
  rdata <- data.frame(names, r)
  rdata <- rdata %>% mutate(language=lang)
  return(rdata)
}

######################

# add lemmas of one CHILDES half not existing at other half and do same size and order
same_size_df <- function(df1, df2) { 
  if (nrow(df1)>3){
  df1 <- df1 %>% 
    drop_na(frequency)
  df2 <- df2 %>% 
    drop_na(frequency)
  firstlistlemma<-(df1$uni_lemma)
    secondlistlemma<-(df2$uni_lemma)
    diff1<-setdiff(firstlistlemma,secondlistlemma) 
    diff2<-setdiff(secondlistlemma,firstlistlemma) 
    df<-as.data.frame(c(diff1, diff2)) %>%
      rename(uni_lemma = "c(diff1, diff2)" )
    df1<- anti_join(df1, df, by='uni_lemma')
    df2<- anti_join(df2, df, by='uni_lemma')
     both=list(tibble(df1[!duplicated(df1$uni_lemma),]), tibble(df2[!duplicated(df2$uni_lemma),]))
  return(both)
     }
     else{
    both=list(tibble(df1), tibble(df2))
    return(both)
  }
}

##########################

# extracts corr number from df
lang_pred_half <- function(lang, pred, split_corpora){
 s=split_corpora %>% filter(language==lang, names==pred)
 r=s[1,2]
return(as.numeric(r) ) 
}

#####################

# measure correlation between two vectors
pred_cor <- function(pred, df){
r<- cor(df[[1]][[pred]], df[[2]][[pred]], method="pearson")
return(r)  
}

#######################  

#get unilemmas for words, and predictors for unilemmas
get_predictors<- function(half, lang){
     token_metrics<- get_childes_metrics(lang=lang, metric_funs, corpus_args, import_data=half)
     wg <- tryCatch({create_inst_data(lang, "WG")}, error = function(e){})
     ws <- tryCatch({create_inst_data(lang, "WS")}, error = function(e){})
     wg_summary <- tryCatch({collapse_inst_data(wg)}, error = function(e){})
     ws_summary <- tryCatch({collapse_inst_data(ws)}, error = function(e){})
     if (!is.null(wg) & !is.null(ws)){
      comb_summary <- combine_form_data(list(wg_summary, ws_summary))
     } else if (!is.null(wg)){
      comb_summary <- wg_summary 
     } else if (!is.null(ws)){
      comb_summary <- ws_summary 
     } else{
      comb_summary <- NULL 
     }
     if (!is.null(comb_summary)){
      uni_lemmas <- get_uni_lemmas(comb_summary)
      uni_metrics<- get_uni_lemma_metrics(lang, build_uni_lemma_map(uni_lemmas), token_metrics)
      uni_metrics <- prepare_frequency(lang, uni_metrics, uni_lemmas, count, count_first, count_last, count_solo, frequency, first_frequency, final_frequency, solo_frequency) 
     return(uni_metrics[order(uni_metrics$uni_lemma),])
     } else{
      uni_metrics <- NA  
     return(uni_metrics)   
     }
}
#############################

#get CHILDES and randomly splits words in half
split_half <-function(lang){ 
  childes_lang <- convert_lang_childes(lang)
  file_t <- file.path(childes_path, glue("{childes_lang}_tokens.rds"))
  if(file.exists(file_t)) {
   tokens <-readRDS(file_t)
  }else{
   tokens <- get_childes_data(childes_lang, corpus_args)[[2]] 
  }
  tokens <- tokens %>% mutate(gloss=tolower(gloss))
  file_u <- file.path(childes_path, glue("{childes_lang}_utterances.rds"))
  if(file.exists(file_u)) {
   utterances <- readRDS(file_u)
  }else{
   utterances <- get_childes_data(childes_lang, corpus_args)[[1]] 
  } 
 
  ind <- sample(c(TRUE, FALSE), nrow(tokens), replace=TRUE, prob=c(0.5, 0.5)) #split tokens
  tokens1 <- tokens[ind, ] #split in two
  tokens2 <- tokens[!ind, ] 
  
   utterances1 <- utterances %>%
     filter(id %in% tokens1$utterance_id) %>%
     mutate(gloss=tolower(gloss))  

    utterances2 <- utterances %>%
     filter(id %in% tokens2$utterance_id) %>%
     mutate(gloss=tolower(gloss)) 
    
  half1 <- list("utterances" = utterances1, "tokens" = tokens1)
  half2 <- list("utterances" = utterances2, "tokens" = tokens2)

  both=list(tibble(get_predictors(half1, lang)), tibble(get_predictors(half2, lang)))
  return(both)  
  }

```



### Measure reliabilities for CHILDES predictors

Implement functiosn above, for each target language, measure (and optionally word_class). The mean correlation r is selected from 5 random splits of the corpus. 

```{r childes, warning = FALSE, message=FALSE}

rel <- file.path(glue("data/reliabilities.rds")) #check whether cached data exist
  if(!file.exists(rel)) {
 
split_corpora <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora1 <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora2 <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora3 <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora4 <- lapply(target_langs, get_main_rel) %>% bind_rows()


reliabilities <- expand_grid(lang = target_langs,
                            word_class = c("all"),
                            pred = predictor_list) %>%  
                            rowwise %>%            
                            mutate(r =  (sum(lang_pred_half(lang, pred, split_corpora), lang_pred_half(lang, pred, split_corpora1), lang_pred_half(lang, pred, split_corpora2), lang_pred_half(lang, pred, split_corpora3), lang_pred_half(lang, pred, split_corpora4)))/5) %>% 
  mutate(sbr = sbformula(as.numeric(r)))

reliabilities %>%
  knitr::kable(digits = 2)

splitpath<- glue("data/reliabilities.rds")
saveRDS(reliabilities, splitpath)
  }else{
reliabilities <-  readRDS(rel)
}


```

### Measure R squared from cross-validation 

The rsq_cv function measures the R squared of the cross-validated test models
```{r x-validation}

rsq <- file.path(glue("data/rsq.rds")) #check whether cached data exist
  if(!file.exists(rsq)) {

#Italian: 25, 308 (bath, sit, a)
rsq_cv<- function(word_values, predictor, lang, meas) {
  r<-word_values |>
  filter(language==lang, measure==meas) |>
  group_nest() |>
  mutate(loo_data = map(data, crossv_loo),
         loo_models = map(data, fit_cv_models, list(make_effs_formula(predictor, FALSE, TRUE))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))
  r_lp<- r |>
    unnest(cv_results) %>% 
    dplyr::select(r2) %>%
   unique()
  r=r_lp[1,1]
  return(as.character(r)) 
  }

file_wv <- file.path(glue("/Users/lscpuser/Documents/aoa-pipeline8/aoa-pipeline-main-1/data/word_values.rds"))
word_values <- readRDS(file_wv) ####data needed from aoa-template
word_values <- word_values %>% 
  filter(!(language=="Russian" & category=="sounds"))

rsq <- expand_grid(  #     class = c("all"),
                   pred = predictor_list,
                   lang = target_langs,
                   meas = c("produces", "understands")) %>% 
                   rowwise %>%
                   mutate(r2= rsq_cv(word_values, pred, lang, meas))

rsqpath<- glue("data/rsq.rds")
saveRDS(rsq, rsqpath)

}else{
rsq<-  readRDS(rsq)
}

```

### Merge to create final corpus 
```{r final corpus}

dfinal <- reliabilities %>%
  left_join(reliabilities_aoa) %>%
  unique() %>%
  filter(!is.na(meas)) %>% 
  mutate(threshold = sbr * sbr_aoa) %>%
  left_join(unique(rsq)) %>%
 # filter(!is.infinite(as.numeric(r2))) %>%
  unique()


dfinalpath<- glue("data/dfinal.rds")
saveRDS(dfinal, dfinalpath)

dfinal %>%
  knitr::kable(digits = 2)
```
